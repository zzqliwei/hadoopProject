package com.westar.streaming;

import org.apache.spark.storage.StorageLevel;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import scala.Tuple2;

import java.util.Arrays;
import java.util.regex.Pattern;

/**
 *  ssc.socketTextStream("localhost",9998, StorageLevel.MEMORY_AND_DISK_SER());
 *  ssc.textFileStream(args[0]);
 *   delimited text (eg. generated by 'nc')
 *
 * WordCount程序，Spark Streaming消费TCP Server发过来的实时数据的例子：
 *
 * 1、在master服务器上启动一个Netcat server
 * `$ nc -lk 9998` (如果nc命令无效的话，我们可以用yum install -y nc来安装nc)
 *
 *
 */
public class JavaLocalNetworkWordCount {
    private static final Pattern SPACE = Pattern.compile(" ");

    public static void main(String[] args) throws InterruptedException {
        // StreamingContext 编程入口
        JavaStreamingContext ssc = new JavaStreamingContext("local[*]",
                "JavaLocalNetworkWordCount", Durations.seconds(1),
                System.getenv("SPARK_HOME"),JavaStreamingContext.jarOfClass(JavaLocalNetworkWordCount.class.getClass()));

        //数据接收器(Receiver)
        //创建一个接收器(JavaReceiverInputDStream)，这个接收器接收一台机器上的某个端口通过socket发送过来的数据并处理
        JavaDStream<String> lines = ssc.socketTextStream("192.168.3.110",9998, StorageLevel.MEMORY_AND_DISK_SER());

        //数据处理(Process)
        //处理的逻辑，就是简单的进行word count
        JavaDStream<String> words = lines.flatMap(x-> Arrays.asList(SPACE.split(x)).iterator());
        JavaPairDStream<String,Integer> wordCounts = words.mapToPair(s
        -> new Tuple2<>(s,1)).reduceByKey((i1,i2) -> i1+i2);
        //结果输出(Output)
        //将结果输出到控制台
        wordCounts.print();
        ssc.start();
        ssc.awaitTermination();

    }
}
